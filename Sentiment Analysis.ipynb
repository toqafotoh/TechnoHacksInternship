{"cells":[{"cell_type":"markdown","metadata":{},"source":["<span style=\"color: #8f3b43; font-size: 35px; font-weight: bold; display: inline-flex; align-items: center;\">\n","  Tweets Sentiment Analysis \n","  <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" alt=\"My Happy SVG\" style=\"height: 35px; margin-left: 10px;\">\n","</span>\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 14640 entries, 0 to 14639\n","Data columns (total 15 columns):\n"," #   Column                        Non-Null Count  Dtype  \n","---  ------                        --------------  -----  \n"," 0   tweet_id                      14640 non-null  int64  \n"," 1   airline_sentiment             14640 non-null  object \n"," 2   airline_sentiment_confidence  14640 non-null  float64\n"," 3   negativereason                9178 non-null   object \n"," 4   negativereason_confidence     10522 non-null  float64\n"," 5   airline                       14640 non-null  object \n"," 6   airline_sentiment_gold        40 non-null     object \n"," 7   name                          14640 non-null  object \n"," 8   negativereason_gold           32 non-null     object \n"," 9   retweet_count                 14640 non-null  int64  \n"," 10  text                          14640 non-null  object \n"," 11  tweet_coord                   1019 non-null   object \n"," 12  tweet_created                 14640 non-null  object \n"," 13  tweet_location                9907 non-null   object \n"," 14  user_timezone                 9820 non-null   object \n","dtypes: float64(2), int64(2), object(11)\n","memory usage: 1.7+ MB\n"]}],"source":["import pandas as pd\n","df = pd.read_csv(\"Tweets.csv\")\n","df.info()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>airline_sentiment</th>\n","      <th>airline_sentiment_confidence</th>\n","      <th>negativereason</th>\n","      <th>negativereason_confidence</th>\n","      <th>airline</th>\n","      <th>airline_sentiment_gold</th>\n","      <th>name</th>\n","      <th>negativereason_gold</th>\n","      <th>retweet_count</th>\n","      <th>text</th>\n","      <th>tweet_coord</th>\n","      <th>tweet_created</th>\n","      <th>tweet_location</th>\n","      <th>user_timezone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>570306133677760513</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>cairdin</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica What @dhepburn said.</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:35:52 -0800</td>\n","      <td>NaN</td>\n","      <td>Eastern Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>570301130888122368</td>\n","      <td>positive</td>\n","      <td>0.3486</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica plus you've added commercials t...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:59 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>570301083672813571</td>\n","      <td>neutral</td>\n","      <td>0.6837</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>yvonnalynn</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:48 -0800</td>\n","      <td>Lets Play</td>\n","      <td>Central Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>570301031407624196</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Bad Flight</td>\n","      <td>0.7033</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica it's really aggressive to blast...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:15:36 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>570300817074462722</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Can't Tell</td>\n","      <td>1.0000</td>\n","      <td>Virgin America</td>\n","      <td>NaN</td>\n","      <td>jnardino</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@VirginAmerica and it's a really big bad thing...</td>\n","      <td>NaN</td>\n","      <td>2015-02-24 11:14:45 -0800</td>\n","      <td>NaN</td>\n","      <td>Pacific Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14635</th>\n","      <td>569587686496825344</td>\n","      <td>positive</td>\n","      <td>0.3487</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>American</td>\n","      <td>NaN</td>\n","      <td>KristenReenders</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@AmericanAir thank you we got on a different f...</td>\n","      <td>NaN</td>\n","      <td>2015-02-22 12:01:01 -0800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14636</th>\n","      <td>569587371693355008</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Customer Service Issue</td>\n","      <td>1.0000</td>\n","      <td>American</td>\n","      <td>NaN</td>\n","      <td>itsropes</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n","      <td>NaN</td>\n","      <td>2015-02-22 11:59:46 -0800</td>\n","      <td>Texas</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14637</th>\n","      <td>569587242672398336</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>American</td>\n","      <td>NaN</td>\n","      <td>sanyabun</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@AmericanAir Please bring American Airlines to...</td>\n","      <td>NaN</td>\n","      <td>2015-02-22 11:59:15 -0800</td>\n","      <td>Nigeria,lagos</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14638</th>\n","      <td>569587188687634433</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>Customer Service Issue</td>\n","      <td>0.6659</td>\n","      <td>American</td>\n","      <td>NaN</td>\n","      <td>SraJackson</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@AmericanAir you have my money, you change my ...</td>\n","      <td>NaN</td>\n","      <td>2015-02-22 11:59:02 -0800</td>\n","      <td>New Jersey</td>\n","      <td>Eastern Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>14639</th>\n","      <td>569587140490866689</td>\n","      <td>neutral</td>\n","      <td>0.6771</td>\n","      <td>NaN</td>\n","      <td>0.0000</td>\n","      <td>American</td>\n","      <td>NaN</td>\n","      <td>daviddtwu</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n","      <td>NaN</td>\n","      <td>2015-02-22 11:58:51 -0800</td>\n","      <td>dallas, TX</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14640 rows Ã— 15 columns</p>\n","</div>"],"text/plain":["                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0      570306133677760513           neutral                        1.0000   \n","1      570301130888122368          positive                        0.3486   \n","2      570301083672813571           neutral                        0.6837   \n","3      570301031407624196          negative                        1.0000   \n","4      570300817074462722          negative                        1.0000   \n","...                   ...               ...                           ...   \n","14635  569587686496825344          positive                        0.3487   \n","14636  569587371693355008          negative                        1.0000   \n","14637  569587242672398336           neutral                        1.0000   \n","14638  569587188687634433          negative                        1.0000   \n","14639  569587140490866689           neutral                        0.6771   \n","\n","               negativereason  negativereason_confidence         airline  \\\n","0                         NaN                        NaN  Virgin America   \n","1                         NaN                     0.0000  Virgin America   \n","2                         NaN                        NaN  Virgin America   \n","3                  Bad Flight                     0.7033  Virgin America   \n","4                  Can't Tell                     1.0000  Virgin America   \n","...                       ...                        ...             ...   \n","14635                     NaN                     0.0000        American   \n","14636  Customer Service Issue                     1.0000        American   \n","14637                     NaN                        NaN        American   \n","14638  Customer Service Issue                     0.6659        American   \n","14639                     NaN                     0.0000        American   \n","\n","      airline_sentiment_gold             name negativereason_gold  \\\n","0                        NaN          cairdin                 NaN   \n","1                        NaN         jnardino                 NaN   \n","2                        NaN       yvonnalynn                 NaN   \n","3                        NaN         jnardino                 NaN   \n","4                        NaN         jnardino                 NaN   \n","...                      ...              ...                 ...   \n","14635                    NaN  KristenReenders                 NaN   \n","14636                    NaN         itsropes                 NaN   \n","14637                    NaN         sanyabun                 NaN   \n","14638                    NaN       SraJackson                 NaN   \n","14639                    NaN        daviddtwu                 NaN   \n","\n","       retweet_count                                               text  \\\n","0                  0                @VirginAmerica What @dhepburn said.   \n","1                  0  @VirginAmerica plus you've added commercials t...   \n","2                  0  @VirginAmerica I didn't today... Must mean I n...   \n","3                  0  @VirginAmerica it's really aggressive to blast...   \n","4                  0  @VirginAmerica and it's a really big bad thing...   \n","...              ...                                                ...   \n","14635              0  @AmericanAir thank you we got on a different f...   \n","14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n","14637              0  @AmericanAir Please bring American Airlines to...   \n","14638              0  @AmericanAir you have my money, you change my ...   \n","14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n","\n","      tweet_coord              tweet_created tweet_location  \\\n","0             NaN  2015-02-24 11:35:52 -0800            NaN   \n","1             NaN  2015-02-24 11:15:59 -0800            NaN   \n","2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n","3             NaN  2015-02-24 11:15:36 -0800            NaN   \n","4             NaN  2015-02-24 11:14:45 -0800            NaN   \n","...           ...                        ...            ...   \n","14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n","14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n","14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n","14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n","14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n","\n","                    user_timezone  \n","0      Eastern Time (US & Canada)  \n","1      Pacific Time (US & Canada)  \n","2      Central Time (US & Canada)  \n","3      Pacific Time (US & Canada)  \n","4      Pacific Time (US & Canada)  \n","...                           ...  \n","14635                         NaN  \n","14636                         NaN  \n","14637                         NaN  \n","14638  Eastern Time (US & Canada)  \n","14639                         NaN  \n","\n","[14640 rows x 15 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["most of the data is an object so "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>airline_sentiment</th>\n","      <th>negativereason</th>\n","      <th>airline</th>\n","      <th>airline_sentiment_gold</th>\n","      <th>name</th>\n","      <th>negativereason_gold</th>\n","      <th>text</th>\n","      <th>tweet_coord</th>\n","      <th>tweet_created</th>\n","      <th>tweet_location</th>\n","      <th>user_timezone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>14640</td>\n","      <td>9178</td>\n","      <td>14640</td>\n","      <td>40</td>\n","      <td>14640</td>\n","      <td>32</td>\n","      <td>14640</td>\n","      <td>1019</td>\n","      <td>14640</td>\n","      <td>9907</td>\n","      <td>9820</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>3</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>7701</td>\n","      <td>13</td>\n","      <td>14427</td>\n","      <td>832</td>\n","      <td>14247</td>\n","      <td>3081</td>\n","      <td>85</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>negative</td>\n","      <td>Customer Service Issue</td>\n","      <td>United</td>\n","      <td>negative</td>\n","      <td>JetBlueNews</td>\n","      <td>Customer Service Issue</td>\n","      <td>@united thanks</td>\n","      <td>[0.0, 0.0]</td>\n","      <td>2015-02-24 09:54:34 -0800</td>\n","      <td>Boston, MA</td>\n","      <td>Eastern Time (US &amp; Canada)</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>9178</td>\n","      <td>2910</td>\n","      <td>3822</td>\n","      <td>32</td>\n","      <td>63</td>\n","      <td>12</td>\n","      <td>6</td>\n","      <td>164</td>\n","      <td>5</td>\n","      <td>157</td>\n","      <td>3744</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       airline_sentiment          negativereason airline  \\\n","count              14640                    9178   14640   \n","unique                 3                      10       6   \n","top             negative  Customer Service Issue  United   \n","freq                9178                    2910    3822   \n","\n","       airline_sentiment_gold         name     negativereason_gold  \\\n","count                      40        14640                      32   \n","unique                      3         7701                      13   \n","top                  negative  JetBlueNews  Customer Service Issue   \n","freq                       32           63                      12   \n","\n","                  text tweet_coord              tweet_created tweet_location  \\\n","count            14640        1019                      14640           9907   \n","unique           14427         832                      14247           3081   \n","top     @united thanks  [0.0, 0.0]  2015-02-24 09:54:34 -0800     Boston, MA   \n","freq                 6         164                          5            157   \n","\n","                     user_timezone  \n","count                         9820  \n","unique                          85  \n","top     Eastern Time (US & Canada)  \n","freq                          3744  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.describe(include=['O'])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer \n","from sklearn.model_selection import GridSearchCV\n","vect = CountVectorizer()\n","param_grid = {\n","    'max_features': [1000, 2000, 3000],\n","    'ngram_range': [(1,1), (1,2), (1,3)],\n","    #unigram only, uni and bigram, uni and trigram\n","    'min_df' : [60,100,40],\n","    'max_df' : [1000,5000,4000]\n","    #ignore any term that frequentlly repeated less than the number you specify\n","}\n","grid_search = GridSearchCV(vect, param_grid, cv=5)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;31mInit signature:\u001b[0m\n","\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.int64'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n","\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mDocstring:\u001b[0m     \n","Convert a collection of text documents to a matrix of token counts.\n","\n","This implementation produces a sparse representation of the counts using\n","scipy.sparse.csr_matrix.\n","\n","If you do not provide an a-priori dictionary and you do not use an analyzer\n","that does some kind of feature selection then the number of features will\n","be equal to the vocabulary size found by analyzing the data.\n","\n","For an efficiency comparision of the different feature extractors, see\n",":ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n","\n","Read more in the :ref:`User Guide <text_feature_extraction>`.\n","\n","Parameters\n","----------\n","input : {'filename', 'file', 'content'}, default='content'\n","    - If `'filename'`, the sequence passed as an argument to fit is\n","      expected to be a list of filenames that need reading to fetch\n","      the raw content to analyze.\n","\n","    - If `'file'`, the sequence items must have a 'read' method (file-like\n","      object) that is called to fetch the bytes in memory.\n","\n","    - If `'content'`, the input is expected to be a sequence of items that\n","      can be of type string or byte.\n","\n","encoding : str, default='utf-8'\n","    If bytes or files are given to analyze, this encoding is used to\n","    decode.\n","\n","decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n","    Instruction on what to do if a byte sequence is given to analyze that\n","    contains characters not of the given `encoding`. By default, it is\n","    'strict', meaning that a UnicodeDecodeError will be raised. Other\n","    values are 'ignore' and 'replace'.\n","\n","strip_accents : {'ascii', 'unicode'} or callable, default=None\n","    Remove accents and perform other character normalization\n","    during the preprocessing step.\n","    'ascii' is a fast method that only works on characters that have\n","    a direct ASCII mapping.\n","    'unicode' is a slightly slower method that works on any characters.\n","    None (default) means no character normalization is performed.\n","\n","    Both 'ascii' and 'unicode' use NFKD normalization from\n","    :func:`unicodedata.normalize`.\n","\n","lowercase : bool, default=True\n","    Convert all characters to lowercase before tokenizing.\n","\n","preprocessor : callable, default=None\n","    Override the preprocessing (strip_accents and lowercase) stage while\n","    preserving the tokenizing and n-grams generation steps.\n","    Only applies if ``analyzer`` is not callable.\n","\n","tokenizer : callable, default=None\n","    Override the string tokenization step while preserving the\n","    preprocessing and n-grams generation steps.\n","    Only applies if ``analyzer == 'word'``.\n","\n","stop_words : {'english'}, list, default=None\n","    If 'english', a built-in stop word list for English is used.\n","    There are several known issues with 'english' and you should\n","    consider an alternative (see :ref:`stop_words`).\n","\n","    If a list, that list is assumed to contain stop words, all of which\n","    will be removed from the resulting tokens.\n","    Only applies if ``analyzer == 'word'``.\n","\n","    If None, no stop words will be used. In this case, setting `max_df`\n","    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n","    and filter stop words based on intra corpus document frequency of terms.\n","\n","token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n","    Regular expression denoting what constitutes a \"token\", only used\n","    if ``analyzer == 'word'``. The default regexp select tokens of 2\n","    or more alphanumeric characters (punctuation is completely ignored\n","    and always treated as a token separator).\n","\n","    If there is a capturing group in token_pattern then the\n","    captured group content, not the entire match, becomes the token.\n","    At most one capturing group is permitted.\n","\n","ngram_range : tuple (min_n, max_n), default=(1, 1)\n","    The lower and upper boundary of the range of n-values for different\n","    word n-grams or char n-grams to be extracted. All values of n such\n","    such that min_n <= n <= max_n will be used. For example an\n","    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n","    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n","    Only applies if ``analyzer`` is not callable.\n","\n","analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n","    Whether the feature should be made of word n-gram or character\n","    n-grams.\n","    Option 'char_wb' creates character n-grams only from text inside\n","    word boundaries; n-grams at the edges of words are padded with space.\n","\n","    If a callable is passed it is used to extract the sequence of features\n","    out of the raw, unprocessed input.\n","\n","    .. versionchanged:: 0.21\n","\n","    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n","    first read from the file and then passed to the given callable\n","    analyzer.\n","\n","max_df : float in range [0.0, 1.0] or int, default=1.0\n","    When building the vocabulary ignore terms that have a document\n","    frequency strictly higher than the given threshold (corpus-specific\n","    stop words).\n","    If float, the parameter represents a proportion of documents, integer\n","    absolute counts.\n","    This parameter is ignored if vocabulary is not None.\n","\n","min_df : float in range [0.0, 1.0] or int, default=1\n","    When building the vocabulary ignore terms that have a document\n","    frequency strictly lower than the given threshold. This value is also\n","    called cut-off in the literature.\n","    If float, the parameter represents a proportion of documents, integer\n","    absolute counts.\n","    This parameter is ignored if vocabulary is not None.\n","\n","max_features : int, default=None\n","    If not None, build a vocabulary that only consider the top\n","    `max_features` ordered by term frequency across the corpus.\n","    Otherwise, all features are used.\n","\n","    This parameter is ignored if vocabulary is not None.\n","\n","vocabulary : Mapping or iterable, default=None\n","    Either a Mapping (e.g., a dict) where keys are terms and values are\n","    indices in the feature matrix, or an iterable over terms. If not\n","    given, a vocabulary is determined from the input documents. Indices\n","    in the mapping should not be repeated and should not have any gap\n","    between 0 and the largest index.\n","\n","binary : bool, default=False\n","    If True, all non zero counts are set to 1. This is useful for discrete\n","    probabilistic models that model binary events rather than integer\n","    counts.\n","\n","dtype : dtype, default=np.int64\n","    Type of the matrix returned by fit_transform() or transform().\n","\n","Attributes\n","----------\n","vocabulary_ : dict\n","    A mapping of terms to feature indices.\n","\n","fixed_vocabulary_ : bool\n","    True if a fixed vocabulary of term to indices mapping\n","    is provided by the user.\n","\n","stop_words_ : set\n","    Terms that were ignored because they either:\n","\n","      - occurred in too many documents (`max_df`)\n","      - occurred in too few documents (`min_df`)\n","      - were cut off by feature selection (`max_features`).\n","\n","    This is only available if no vocabulary was given.\n","\n","See Also\n","--------\n","HashingVectorizer : Convert a collection of text documents to a\n","    matrix of token counts.\n","\n","TfidfVectorizer : Convert a collection of raw documents to a matrix\n","    of TF-IDF features.\n","\n","Notes\n","-----\n","The ``stop_words_`` attribute can get large and increase the model size\n","when pickling. This attribute is provided only for introspection and can\n","be safely removed using delattr or set to None before pickling.\n","\n","Examples\n","--------\n",">>> from sklearn.feature_extraction.text import CountVectorizer\n",">>> corpus = [\n","...     'This is the first document.',\n","...     'This document is the second document.',\n","...     'And this is the third one.',\n","...     'Is this the first document?',\n","... ]\n",">>> vectorizer = CountVectorizer()\n",">>> X = vectorizer.fit_transform(corpus)\n",">>> vectorizer.get_feature_names_out()\n","array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n","       'this'], ...)\n",">>> print(X.toarray())\n","[[0 1 1 1 0 0 1 0 1]\n"," [0 2 0 1 0 1 1 0 1]\n"," [1 0 0 1 1 0 1 1 1]\n"," [0 1 1 1 0 0 1 0 1]]\n",">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",">>> X2 = vectorizer2.fit_transform(corpus)\n",">>> vectorizer2.get_feature_names_out()\n","array(['and this', 'document is', 'first document', 'is the', 'is this',\n","       'second document', 'the first', 'the second', 'the third', 'third one',\n","       'this document', 'this is', 'this the'], ...)\n"," >>> print(X2.toarray())\n"," [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n"," [0 1 0 1 0 1 0 1 0 0 1 0 0]\n"," [1 0 0 1 0 0 0 0 1 1 0 1 0]\n"," [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n","\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\sklearn\\feature_extraction\\text.py\n","\u001b[1;31mType:\u001b[0m           type\n","\u001b[1;31mSubclasses:\u001b[0m     TfidfVectorizer"]}],"source":["CountVectorizer?"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from nltk import word_tokenize "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["['@', 'VirginAmerica', 'What', '@', 'dhepburn', 'said', '.']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["word_tokenize(df.text[0])\n","#it is a function that spilt each word and even punctaution into a separated element"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["tokens_list = [word_tokenize(text) for text in df.text]\n","len_tokens = []\n","for i in range(len(tokens_list)):\n","    len_tokens.append(len(tokens_list[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# a feature with punctuation and many word is a very emotionlly charged opinion "]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["df['n_tokens'] = len_tokens"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 's' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\user\\vs code\\TechnoHacksInternship\\TechnoHacksInternship\\Sentiment Analysis.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/vs%20code/TechnoHacksInternship/TechnoHacksInternship/Sentiment%20Analysis.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m s\n","\u001b[1;31mNameError\u001b[0m: name 's' is not defined"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
